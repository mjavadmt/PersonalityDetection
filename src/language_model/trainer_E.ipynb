{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29184,"status":"ok","timestamp":1688663671888,"user":{"displayName":"mjavad mt","userId":"11882346090839832004"},"user_tz":-210},"id":"7gp3_1-suJmk","outputId":"0a5307fa-b4d0-4fe8-d710-32f64e5f9b5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers[torch]\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/7.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/7.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.14.1 (from transformers[torch])\n","  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 (from transformers[torch])\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors\u003e=0.3.1 (from transformers[torch])\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n","Requirement already satisfied: torch!=1.12.0,\u003e=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n","Collecting accelerate\u003e=0.20.2 (from transformers[torch])\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate\u003e=0.20.2-\u003etransformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.14.1-\u003etransformers[torch]) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (16.0.6)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (1.26.16)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers[torch]) (3.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch!=1.12.0,\u003e=1.9-\u003etransformers[torch]) (1.3.0)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate\n","Successfully installed accelerate-0.20.3 huggingface-hub-0.16.2 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.20.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.6.0-\u003eaccelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.6.0-\u003eaccelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.6.0-\u003eaccelerate) (16.0.6)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.6.0-\u003eaccelerate) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.6.0-\u003eaccelerate) (1.3.0)\n"]}],"source":["!pip install transformers[torch]\n","!pip install accelerate -U"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"executionInfo":{"elapsed":1,"status":"ok","timestamp":1688663708941,"user":{"displayName":"mjavad mt","userId":"11882346090839832004"},"user_tz":-210},"id":"qxKSiitaswFG"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, random_split\n","from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1688663708958,"user":{"displayName":"mjavad mt","userId":"11882346090839832004"},"user_tz":-210},"id":"2XNzFaU7swFK","outputId":"633a4c54-1fbb-4f02-f78b-abaf71f66189"},"outputs":[{"data":{"text/plain":["\u003ctorch._C.Generator at 0x7f9b8bf8f690\u003e"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(42)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1688663708943,"user":{"displayName":"mjavad mt","userId":"11882346090839832004"},"user_tz":-210},"id":"4ILHODDIswFL"},"outputs":[],"source":["import random\n","random.seed(1234)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":104},"id":"3ORlraRMswFL"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57ea2ae56a3f4753a248663deb563bca","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.41M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"debb6da274b4440da3c6965b34f65fd1","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.07M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ae0a049a85447e5905abf63f1eb996c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/921 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5adf636754841598a2c28f8842aeafa","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/1.44G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Embedding(50001, 1024)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = GPT2Tokenizer.from_pretrained('flax-community/gpt2-medium-persian', bos_token='\u003c|startoftext|\u003e',\n","                                          eos_token='\u003c|endoftext|\u003e', pad_token='\u003c|pad|\u003e')\n","model = GPT2LMHeadModel.from_pretrained('flax-community/gpt2-medium-persian').cuda()\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Gfbl74ztHx4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QJ2QKwwswFM"},"outputs":[],"source":["df = pd.read_json(\"/content/drive/MyDrive/NLP/Project/datasets.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqhSRbcky4hK"},"outputs":[],"source":["def make_trait(row):\n","    row[\"trait_0\"] = row[\"mbti_result\"][0]\n","    row[\"trait_1\"] = row[\"mbti_result\"][1]\n","    row[\"trait_2\"] = row[\"mbti_result\"][2]\n","    row[\"trait_3\"] = row[\"mbti_result\"][3]\n","    return row\n","\n","df = df.apply(make_trait, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foj8ylSZswFN"},"outputs":[],"source":["class PersonalityDataset(Dataset):\n","    def __init__(self, txt_list, tokenizer, max_length):\n","        self.input_ids = []\n","        self.attn_masks = []\n","        self.labels = []\n","        for txt in txt_list:\n","            encodings_dict = tokenizer('\u003c|startoftext|\u003e' + txt + '\u003c|endoftext|\u003e', truncation=True,\n","                                       max_length=max_length, padding=\"max_length\")\n","            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2060642,"status":"ok","timestamp":1688656755555,"user":{"displayName":"mjavad mt","userId":"11882346090839832004"},"user_tz":-210},"id":"HfadiDaHzOYw","outputId":"11552af4-0f26-40fe-cbbd-136692fb0a5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["training model on trait E ...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    \u003cdiv\u003e\n","      \n","      \u003cprogress value='3801' max='3801' style='width:300px; height:20px; vertical-align: middle;'\u003e\u003c/progress\u003e\n","      [3801/3801 34:14, Epoch 3/3]\n","    \u003c/div\u003e\n","    \u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n"," \u003ctr style=\"text-align: left;\"\u003e\n","      \u003cth\u003eStep\u003c/th\u003e\n","      \u003cth\u003eTraining Loss\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e100\u003c/td\u003e\n","      \u003ctd\u003e6.223100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e200\u003c/td\u003e\n","      \u003ctd\u003e2.636500\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e300\u003c/td\u003e\n","      \u003ctd\u003e2.437500\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e400\u003c/td\u003e\n","      \u003ctd\u003e2.365600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e500\u003c/td\u003e\n","      \u003ctd\u003e2.340300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e600\u003c/td\u003e\n","      \u003ctd\u003e2.308200\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e700\u003c/td\u003e\n","      \u003ctd\u003e2.280900\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e800\u003c/td\u003e\n","      \u003ctd\u003e2.303900\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e900\u003c/td\u003e\n","      \u003ctd\u003e2.266600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1000\u003c/td\u003e\n","      \u003ctd\u003e2.260500\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1100\u003c/td\u003e\n","      \u003ctd\u003e2.264700\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1200\u003c/td\u003e\n","      \u003ctd\u003e2.242600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1300\u003c/td\u003e\n","      \u003ctd\u003e2.215900\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1400\u003c/td\u003e\n","      \u003ctd\u003e2.146700\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1500\u003c/td\u003e\n","      \u003ctd\u003e2.231300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1600\u003c/td\u003e\n","      \u003ctd\u003e2.160200\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1700\u003c/td\u003e\n","      \u003ctd\u003e2.154600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1800\u003c/td\u003e\n","      \u003ctd\u003e2.234300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e1900\u003c/td\u003e\n","      \u003ctd\u003e2.146700\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2000\u003c/td\u003e\n","      \u003ctd\u003e2.199100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2100\u003c/td\u003e\n","      \u003ctd\u003e2.099400\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2200\u003c/td\u003e\n","      \u003ctd\u003e2.153900\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2300\u003c/td\u003e\n","      \u003ctd\u003e2.099200\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2400\u003c/td\u003e\n","      \u003ctd\u003e2.125000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2500\u003c/td\u003e\n","      \u003ctd\u003e2.093100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2600\u003c/td\u003e\n","      \u003ctd\u003e2.087500\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2700\u003c/td\u003e\n","      \u003ctd\u003e2.107700\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2800\u003c/td\u003e\n","      \u003ctd\u003e2.099600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e2900\u003c/td\u003e\n","      \u003ctd\u003e2.098000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3000\u003c/td\u003e\n","      \u003ctd\u003e2.095900\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3100\u003c/td\u003e\n","      \u003ctd\u003e2.098600\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3200\u003c/td\u003e\n","      \u003ctd\u003e2.148400\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3300\u003c/td\u003e\n","      \u003ctd\u003e1.996100\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3400\u003c/td\u003e\n","      \u003ctd\u003e2.111800\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3500\u003c/td\u003e\n","      \u003ctd\u003e2.095000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3600\u003c/td\u003e\n","      \u003ctd\u003e2.062300\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3700\u003c/td\u003e\n","      \u003ctd\u003e2.060400\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003ctd\u003e3800\u003c/td\u003e\n","      \u003ctd\u003e2.076500\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\u003cp\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["for trait in range(1):\n","    curr_label_str = f\"trait_{trait}\"\n","    curr_df = df[[\"tweets\", curr_label_str]]\n","    grouped_curr_df = curr_df.groupby(curr_label_str)\n","    for label, frame in grouped_curr_df:\n","      if label == \"I\":\n","        continue\n","      print(f\"training model on trait {label} ...\")\n","      top_k = 30\n","      max_length = 750\n","      tweets = frame[\"tweets\"].apply(lambda x : \" \".join(random.sample(x, top_k))[:max_length])\n","      max_length = max([len(tokenizer.encode(tweet)) for tweet in tweets])\n","      dataset = PersonalityDataset(tweets, tokenizer, max_length=max_length)\n","      train_size = int(0.9 * len(dataset))\n","      train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n","      training_args = TrainingArguments(output_dir='./models', num_train_epochs=2, logging_steps=100, save_steps=5000,\n","                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n","                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')\n","      Trainer(model=model,  args=training_args, train_dataset=train_dataset,\n","        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n","                                                              'attention_mask': torch.stack([f[1] for f in data]),\n","                                                              'labels': torch.stack([f[0] for f in data])}).train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DcabwloswFP"},"outputs":[],"source":["model.save_pretrained(\"/content/drive/MyDrive/NLP/E.language_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tqn9u4nJ83I3"},"outputs":[],"source":["load_model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/NLP/E.language_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15491,"status":"ok","timestamp":1688657188196,"user":{"displayName":"mjavad mt","userId":"11882346090839832004"},"user_tz":-210},"id":"Me9AO7_zrkuU","outputId":"f57cfa1c-f5c5-48ec-9863-ba31d466553d"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:5 for open-end generation.\n"]}],"source":["generated = tokenizer(\"\u003c|startoftext|\u003e \", return_tensors=\"pt\").input_ids.cuda()\n","\n","load_model = load_model.cuda()\n","\n","sample_outputs = load_model.generate(generated, do_sample=True, top_k=50,\n","                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=20)\n","\n","with open(\"/content/drive/MyDrive/NLP/E_example.txt\", \"a+\", encoding=\"utf-8\") as f:\n","  for i, sample_output in enumerate(sample_outputs):\n","      f.write(\"{}: {}\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Esh0EwpMytNO"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}