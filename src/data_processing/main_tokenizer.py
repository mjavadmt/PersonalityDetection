from tokenizing.tokenizer import word_tokenizer, sentence_tokenizer

word_tokenizer()
sentence_tokenizer()
